# Fine-Tuning Vision Transformer на Fashion-MNIST

## Архитектура и гиперпараметры

### Модель
- **Базовая архитектура**: Vision Transformer (ViT) - `WinKawaks/vit-small-patch16-224`
- **Стратегия fine-tuning**: Заморозка энкодера, обучение только головы классификатора

### Гиперпараметры
- **Learning rate**: 2e-4 с линейным затуханием
- **Batch size**: 32
- **Эпохи**: 2
- **Оптимизатор**: AdamW
- **Размер изображений**: 224×224 (ресайз с 28×28)

### Датасет
- **Fashion-MNIST**: 10 классов одежды
- **Размеры выборок** (уменьшил количество элементов в датасете для ускорения обучения в учебных условиях):
  - Train: 5,400 изображений
  - Validation: 600 изображений  
  - Test: 1,000 изображений

## Графики обучения

### Динамика потерь (Loss)