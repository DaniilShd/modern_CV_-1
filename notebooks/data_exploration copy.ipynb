{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "591d9092",
   "metadata": {},
   "source": [
    "# Задание по современным методам компьютерного зрения\n",
    "\n",
    "## Тема: \"Fine-tuning и анализ внимания в Vision Transformer\" ##\n",
    "## Студент: Шайдуров Даниил Сергеевич ## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8649dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import ViTImageProcessor\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torchvision.transforms import RandomHorizontalFlip, RandomRotation, ColorJitter, Compose\n",
    "import evaluate \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491d1f9f",
   "metadata": {},
   "source": [
    "# Подготовка данных и fine-tuning ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bef0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "def get_data_loaders(batch_size=32):\n",
    "    dataset = load_dataset(\"cifar10\")\n",
    "\n",
    "    # Используем процессор от предобученной модели\n",
    "    processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "    # Определяем трансформации\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=processor.image_mean, std=processor.image_std),\n",
    "    ])\n",
    "\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=processor.image_mean, std=processor.image_std),\n",
    "    ])\n",
    "\n",
    "    def preprocess_train(example_batch):\n",
    "        example_batch['pixel_values'] = [train_transforms(image.convert('RGB')) for image in example_batch['img']]\n",
    "        return example_batch\n",
    "\n",
    "    def preprocess_val(example_batch):\n",
    "        example_batch['pixel_values'] = [val_transforms(image.convert('RGB')) for image in example_batch['img']]\n",
    "        return example_batch\n",
    "\n",
    "    # Применяем трансформации и устанавливаем формат для PyTorch\n",
    "    train_dataset = dataset['train'].with_transform(preprocess_train)\n",
    "    split_dataset = train_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    train_ds = split_dataset['train']\n",
    "    val_ds = split_dataset['test']\n",
    "    test_ds = dataset['test'].with_transform(preprocess_val)\n",
    "\n",
    "    # Создаем DataLoader\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader, dataset['test'].features['label'].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff1b94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6b45c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50573c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38917f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb0a495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем датасет CIFAR-10\n",
    "dataset = load_dataset('cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28c6fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем отдельные тестовую и валидационную выборки\n",
    "split = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "train_ds = split['train']\n",
    "val_ds = split['test']  # Это валидационная выборка\n",
    "test_ds = dataset['test']  # Это тестовая выборка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0baf1169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = dataset['train'][255]\n",
    "\n",
    "img = sample['img']\n",
    "label = sample['label']\n",
    "classes = dataset['train'].features['label'].names\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(img)\n",
    "plt.title(f'Label: {classes[label]}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a1423da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.PngImagePlugin.PngImageFile"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a779338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем процессор для предобученной модели ViT\n",
    "model_name = 'google/vit-base-patch16-224'\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b21610c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем аугментации для тренировочной выборки\n",
    "train_transforms = Compose([\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    RandomRotation(degrees=10),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65fe62e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_train(examples):\n",
    "    # ViT ожидает изображения в формате PIL\n",
    "    images = [image.convert('RGB') for image in examples['img']]\n",
    "    # Применяем аугментации\n",
    "    images = [train_transforms(image) for image in images]\n",
    "    # Применяем стандартный процессор ViT (изменение размера, нормализация)\n",
    "    inputs = processor(images=images, return_tensors='pt')\n",
    "    inputs['labels'] = examples['label']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d509ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_val(examples):\n",
    "    images = [image.convert('RGB') for image in examples['img']]\n",
    "    inputs = processor(images=images, return_tensors='pt')\n",
    "    inputs['labels'] = examples['label']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6caadb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применяем трансформации к датасетам\n",
    "train_ds.set_transform(transform_train)\n",
    "val_ds.set_transform(transform_val)\n",
    "test_ds.set_transform(transform_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "643cc2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер тренировочной выборки: 45000\n",
      "Размер валидационной выборки: 5000\n",
      "Размер тестовой выборки: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Размер тренировочной выборки: {len(train_ds)}\")\n",
    "print(f\"Размер валидационной выборки: {len(val_ds)}\")\n",
    "print(f\"Размер тестовой выборки: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f9e730",
   "metadata": {},
   "source": [
    "# Реализация модели и обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa62098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b2676fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем метрику accuracy из библиотеки evaluate\n",
    "accuracy_metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ec0b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e5f6164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Загружаем предобученную модель ViT\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=10,\n",
    "    ignore_mismatched_sizes=True  # Игнорируем несовпадение размера головы классификатора\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcf35fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем гиперпараметры обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./vit-base-cifar10-results',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    "    report_to=None,\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4093443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_370665/2527621201.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Создаем Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52d8c3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Свободно памяти: 0.33 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Очистка памяти\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Проверка свободной памяти\n",
    "print(f\"Свободно памяти: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f514501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаем обучение...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='264' max='14065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  264/14065 02:36 < 2:17:27, 1.67 it/s, Epoch 0.09/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Запускаем fine-tuning!\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mНачинаем обучение...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m train_results = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mОбучение завершено!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Сохраняем модель\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learn/Semestr_3/Modern_CV/Fine-tuning/.venv/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learn/Semestr_3/Modern_CV/Fine-tuning/.venv/lib/python3.12/site-packages/transformers/trainer.py:2679\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2674\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2683\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Запускаем fine-tuning!\n",
    "print(\"Начинаем обучение...\")\n",
    "train_results = trainer.train()\n",
    "print(\"Обучение завершено!\")\n",
    "\n",
    "# Сохраняем модель\n",
    "trainer.save_model()\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865eb16b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tuning-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
